{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment no 3\n",
    "Name: Vishal Pattar \\\n",
    "Roll no: 33557 \\\n",
    "Class: TE AIML \\\n",
    "Subject: Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    W1 = np.random.uniform(-1, 1, (hidden_size, input_size))\n",
    "    b1 = np.random.uniform(-1, 1, (hidden_size, 1))\n",
    "    W2 = np.random.uniform(-1, 1, (output_size, hidden_size))\n",
    "    b2 = np.random.uniform(-1, 1, (output_size, 1))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return A1, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, A2):\n",
    "    m = Y.shape[1]\n",
    "    loss = -1/m * np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, A1, A2, W1, W2, b1, b2):\n",
    "    m = Y.shape[1]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * A1 * (1 - A1)\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, hidden_size, output_size, learning_rate, num_epochs):\n",
    "    input_size = X.shape[0]\n",
    "    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        A1, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "        loss = compute_loss(Y, A2)\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X, Y, A1, A2, W1, W2, b1, b2)\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, b1, W2, b2):\n",
    "    _, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "    predictions = np.round(A2)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7142237056049177\n",
      "Epoch 100, Loss: 0.7031915944430007\n",
      "Epoch 200, Loss: 0.698463991569022\n",
      "Epoch 300, Loss: 0.6964152045892097\n",
      "Epoch 400, Loss: 0.6955034974386278\n",
      "Epoch 500, Loss: 0.6950757504977894\n",
      "Epoch 600, Loss: 0.6948548841482297\n",
      "Epoch 700, Loss: 0.6947230154071705\n",
      "Epoch 800, Loss: 0.6946298408483964\n",
      "Epoch 900, Loss: 0.6945538309487063\n",
      "Epoch 1000, Loss: 0.694485740241544\n",
      "Epoch 1100, Loss: 0.6944215794059079\n",
      "Epoch 1200, Loss: 0.6943596125613116\n",
      "Epoch 1300, Loss: 0.6942990717421716\n",
      "Epoch 1400, Loss: 0.6942396056056364\n",
      "Epoch 1500, Loss: 0.6941810426660657\n",
      "Epoch 1600, Loss: 0.6941232894715967\n",
      "Epoch 1700, Loss: 0.694066286751136\n",
      "Epoch 1800, Loss: 0.6940099904940483\n",
      "Epoch 1900, Loss: 0.6939543637647246\n",
      "Epoch 2000, Loss: 0.6938993731439687\n",
      "Epoch 2100, Loss: 0.6938449871668405\n",
      "Epoch 2200, Loss: 0.6937911756232036\n",
      "Epoch 2300, Loss: 0.6937379092318493\n",
      "Epoch 2400, Loss: 0.6936851594770155\n",
      "Epoch 2500, Loss: 0.6936328985160126\n",
      "Epoch 2600, Loss: 0.6935810991184581\n",
      "Epoch 2700, Loss: 0.6935297346199922\n",
      "Epoch 2800, Loss: 0.6934787788830179\n",
      "Epoch 2900, Loss: 0.6934282062611972\n",
      "Epoch 3000, Loss: 0.6933779915662478\n",
      "Epoch 3100, Loss: 0.6933281100363704\n",
      "Epoch 3200, Loss: 0.6932785373059775\n",
      "Epoch 3300, Loss: 0.6932292493765464\n",
      "Epoch 3400, Loss: 0.6931802225884836\n",
      "Epoch 3500, Loss: 0.6931314335939222\n",
      "Epoch 3600, Loss: 0.6930828593303809\n",
      "Epoch 3700, Loss: 0.6930344769952287\n",
      "Epoch 3800, Loss: 0.6929862640208985\n",
      "Epoch 3900, Loss: 0.6929381980508018\n",
      "Epoch 4000, Loss: 0.6928902569158928\n",
      "Epoch 4100, Loss: 0.6928424186118376\n",
      "Epoch 4200, Loss: 0.6927946612767417\n",
      "Epoch 4300, Loss: 0.6927469631693981\n",
      "Epoch 4400, Loss: 0.6926993026480105\n",
      "Epoch 4500, Loss: 0.6926516581493566\n",
      "Epoch 4600, Loss: 0.6926040081683504\n",
      "Epoch 4700, Loss: 0.692556331237971\n",
      "Epoch 4800, Loss: 0.6925086059095211\n",
      "Epoch 4900, Loss: 0.6924608107331818\n",
      "Epoch 5000, Loss: 0.6924129242388336\n",
      "Epoch 5100, Loss: 0.6923649249171078\n",
      "Epoch 5200, Loss: 0.6923167912006448\n",
      "Epoch 5300, Loss: 0.6922685014455241\n",
      "Epoch 5400, Loss: 0.6922200339128408\n",
      "Epoch 5500, Loss: 0.6921713667504008\n",
      "Epoch 5600, Loss: 0.6921224779745085\n",
      "Epoch 5700, Loss: 0.6920733454518193\n",
      "Epoch 5800, Loss: 0.692023946881234\n",
      "Epoch 5900, Loss: 0.6919742597758098\n",
      "Epoch 6000, Loss: 0.6919242614446641\n",
      "Epoch 6100, Loss: 0.6918739289748465\n",
      "Epoch 6200, Loss: 0.6918232392131597\n",
      "Epoch 6300, Loss: 0.691772168747904\n",
      "Epoch 6400, Loss: 0.6917206938905242\n",
      "Epoch 6500, Loss: 0.6916687906571403\n",
      "Epoch 6600, Loss: 0.6916164347499378\n",
      "Epoch 6700, Loss: 0.6915636015384004\n",
      "Epoch 6800, Loss: 0.691510266040362\n",
      "Epoch 6900, Loss: 0.6914564029028636\n",
      "Epoch 7000, Loss: 0.69140198638279\n",
      "Epoch 7100, Loss: 0.6913469903272729\n",
      "Epoch 7200, Loss: 0.6912913881538392\n",
      "Epoch 7300, Loss: 0.6912351528302875\n",
      "Epoch 7400, Loss: 0.6911782568542759\n",
      "Epoch 7500, Loss: 0.6911206722326038\n",
      "Epoch 7600, Loss: 0.6910623704601699\n",
      "Epoch 7700, Loss: 0.691003322498592\n",
      "Epoch 7800, Loss: 0.6909434987544721\n",
      "Epoch 7900, Loss: 0.6908828690572906\n",
      "Epoch 8000, Loss: 0.6908214026369157\n",
      "Epoch 8100, Loss: 0.6907590681007126\n",
      "Epoch 8200, Loss: 0.6906958334102408\n",
      "Epoch 8300, Loss: 0.6906316658575212\n",
      "Epoch 8400, Loss: 0.6905665320408665\n",
      "Epoch 8500, Loss: 0.6905003978402566\n",
      "Epoch 8600, Loss: 0.6904332283922526\n",
      "Epoch 8700, Loss: 0.6903649880644326\n",
      "Epoch 8800, Loss: 0.6902956404293452\n",
      "Epoch 8900, Loss: 0.6902251482379655\n",
      "Epoch 9000, Loss: 0.6901534733926503\n",
      "Epoch 9100, Loss: 0.6900805769195788\n",
      "Epoch 9200, Loss: 0.6900064189406767\n",
      "Epoch 9300, Loss: 0.6899309586450164\n",
      "Epoch 9400, Loss: 0.6898541542596871\n",
      "Epoch 9500, Loss: 0.689775963020133\n",
      "Epoch 9600, Loss: 0.6896963411399559\n",
      "Epoch 9700, Loss: 0.6896152437801837\n",
      "Epoch 9800, Loss: 0.6895326250180014\n",
      "Epoch 9900, Loss: 0.6894484378149495\n",
      "Predictions: [[1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10000\n",
    "\n",
    "W1, b1, W2, b2 = train(X, Y, hidden_size, output_size, learning_rate, num_epochs)\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict(X, W1, b1, W2, b2)\n",
    "print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
